* TODOs
** PETS
*** TODO how many DOFs can we control? how does that scale? e.g. Half-cheetah vs minitaur vs hexapod vs humanoid
-5 it works (5 Dof arm)
-6 (Half-cheetah) 
*** TODO is the cross-entropy optimization helping? how much? is it necessary? what about replacing it with CMA-ES (or a simplified CMA-ES variant)?
*** TODO what control frequency can we achieve depending on the horizon?
Env: action space x H : frequency on GPU (CPU) : portable on GPU (CPU)
**** 5-Dof Arm: 5 x 5 : ~40Hz (4Hz) : 2Hz (0.5Hz)
**** Half-Cheetah (pop 500): 
-6 x 30: ~6Hz
-6 x 20: ~10Hz
-6 x 20 (5 particles): 16Hz
-6 x 30: 12Hz
*** TODO do we need the uncertainty?
*** TODO how much faster is the neural network versus a true physics engine (pybullet and maybe PhysX or even RaiSim)?
*** TODO what is the model quality compared to the true model? does this technique learn an actual model (capable of good predictions) or something that only works for a specific trajectory?
*** TODO how much do we gain with a GPU? (in term of control frequency)
*** TODO what about actual trajectories (not only moving forward)

** Questions about meta-learning (more to be added better)
*** TODO can we reproduce their result for an interesting robot (e.g. minitaur)?
*** TODO can we add the uncertainty?
*** TODO does it really work? how more data-efficient is meta-learning vs learning from scratch? 
*** TODO how good is the model compared to a model learned from scratch?
*** TODO how well does it scale to more complex robots?

* To remember
** Handful of trials, PETS, Chua et al.
Comme ils évaluent en plottant la plus grande réwarsd trouvée so far, il ne font pas vraiment du mpc controller mais du RL/planning qui ne consiste quand trouvé une bonne séquence d'action. 
* Papers
** General
*** Ensemble Model Patching: A parameter-efficient Variational Bayesian Neurla Network
[[https://arxiv.org/pdf/1905.09453.pdf][pdf]] 
do a mix between droppout and explicit ensemble to have an estimation of the confidence, comapre to explicit ensemble it does not multiply the number of parameters 
and compare to droppout it does not interfere woth batch normalization and does not have much more parameters. 
-> more important for Huge NN
*** new paper by levin inplicit MAML (méta-learning)
- they use explicit L2 regularisation for the inner-loop optimisation
- they reduce the memory complexity by using an implicite outer-gradient approximation isntead of differentiate through the inner-loop gradient step.
*** Meta-Learning Representations for Continual Learning
[[https://arxiv.org/pdf/1905.12588.pdf][pdf]]
they cut the model in two:
- the first part learn the representation and is only updated in the outer (meta) loop which encourage the weights to move to a good representation to avoid interference and encourage generalization
- the second part is train both in the inner and outer loop for eahc task (this part is similar to MAML)
*** EXPLORING MODEL-BASED PLANNING WITH POLICY NETWORKS
[[https://openreview.net/pdf?id=H1exf64KwH][pdf]]
use eitheir a policy network to generate the first generation of the CEM or improve the parameter of the policy network to select the action sequence to perform 
*** Deep Dynamics Models for Learning  Dexterous Manipulation
[[https://sites.google.com/view/pddm/][site]] [[https://arxiv.org/pdf/1909.11652.pdf][pdf]]
**** bootstrap sampling
thay say thant in handful of trials, they say that you do not need to do bootsrap sampling to train the ensemble but using different initialization and different batch is enough.
**** MPC
They use filtering and reward-Weighed Refinement which smooth the action candidates
*** Deep Evidential Regression
[[https://openreview.net/pdf?id=S1eSoeSYwr][pdf]] 
They use an higher probavbilistic prior to model the epistemic uncertainty learn by the NN.
It allows the network to effectivly model that it does not know outside the training examples.
It better estimate the epistemic uncertainty without requiring several sampling.

*** The Differentiable Cross-entropy Method
[[https://arxiv.org/pdf/1909.12830.pdf][pdf]] 
One key idea is to learn a latent action space to not waist time in sampling irrelevant action sequences. 
DCEM learn an encoder a=f_dec(z). They also gradient descent to optimize the latent action sequence (?)
*** ADVANTAGE-WEIGHTED REGRESSION: SIMPLE AND SCALABLE OFF-POLICY REINFORCEMENT LEARNING
[[https://xbpeng.github.io/projects/AWR/2019_AWR.pdf][pdf]]
Simple but effective model-free RL algorithm which alternate between collecting data from the current policy, training (supervise) value function on the data collected 
and training a policy using advantage weighted regression.
*** Model Predictive Path Integral Control using Covariance Variable Importance Sampling
[[https://arxiv.org/pdf/1509.01149.pdf][pdf]]
Introduction of MPC with sampling on GPU. The key ideas are:
Using parallel sampling on gpu in real time (50Hz)
Adapting the variance of the sampling distribution using derivation of the likelihood ratio.
Theu start from a working action sequence.
*** TODO On Learning Symmetric Locomotion
[[https://www.cs.ubc.ca/~van/papers/2019-MIG-symmetry/2019-MIG-symmetry.pdf][pdf]]
*** TODO Reconciling modern machine-learning practice and the classical bias–variance trade-off
[[https://www.pnas.org/content/pnas/116/32/15849.full.pdf][pdf]]
** Minitaur
*** [[https://arxiv.org/pdf/1804.10332.pdf][Sim-to-Real: Learning Agile Locomotion For Quadruped Robots]] 
**** observation space
roll, pitch and angular velocities along these two axes + 8 motor angles, to much noise in the others and compact is better
**** action space
for each leg a=(s,e) swing and extension map to (e+s,e-s) in motor space, better than the 8 angles because it can covert most part of the accessible space in a rectangle
***** first locomotion task
s in [-0.5, 0.5] and e in [pi/2-0.5,pi/2 +0.5]
***** galopping 
****** a(t)
s(t) = 0.3sin(4pit) 
e(t) = 0.35sint(4pit) + 2 
****** pi(o)
s in [-0.25, 0.25] and e in [pi/2-0.25,pi/2 + 0.25]
**** reward
velocity (pos_t-pos_t-1)-w*dt*|torque*velocities|
the episode ends after 1000steps or when the robot loss balance, it tilts more than 0.5 radians 
**** reality gap
better urdf file
better actuator model
simulate latency in the simulation
dynamics randomization (such as friction)
random force apply to the base every 200 steps to force the model to learn how to keep balance
***** compact observation space
they reduce to a size of 4 to reduce the gap (the expected return in simulation drop but, the real life one increase).
If the observation space is too large the learn representation as more chance to be sparse and the real observation have less chance to be apart form it
*** [[https://hal.inria.fr/hal-02084619/document][Learning and adapting quadruped gaits with the ”Intelligent Trial & Error” algorithm]] 
The high dimension action space is reduced to a low dimension policy space using 3 gait parameters for each 2d cartesian position of the legs (24 dimension) which is
further reduced to 16 dimension using the torque send to each legs in each quarter of the cycle. They first use the simulation to find 16,000 high performing policy to cover 
the low dimensional space. For the real robot they use Gaussian Processes to model the performance of each policy in order to find the more confident one and adapt quiwcly.  
*** [[https://bair.berkeley.edu/blog/2018/12/14/sac/][Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots 
]][[https://arxiv.org/pdf/1812.11103.pdf][pdf]] 
they use the model-free RL Soft-Actor Critic (SAC) 
on simulation (from a fined tune one) they have a reward of maximum 200 in 0.3M samples and 80 in 0.15M in real world
**** reward 
(x_t-x_t-1) - 0.05*|joint accelerations| - 0.5*|roll angle of the base| + penalty when the front legs are folded under the robot 
*** [[https://arxiv.org/pdf/1909.12995.pdf][Learning Fast Adaptation with Meta Strategy Optimization]]
They use Meta Strategy Optimisation of a policy taking as inputs the "physical context", the alternate the optimisation of the physical context and the policy parameters theta.
15 episodes are enough to adapt effectivly
They also use randomize parameter: mass, friction, intertia, motor sthrenght, latency, battery votlage, contact friction, joint friction
rewatd are between 1 and 4.
**** params
50Hz - dt=0.02
use PD (P gain 0.5, D gain 0.005)
Obs = (motor angles, roll, pitch, vroll, vpitch) 
r = clip((p_t-p_t-1)/dt,-1,1)
episode length 250steps = 5s
* 2019
** Septembre
*** 04/09/2019
**** installation pybullet
***** with pybullet-gym
il n'y a pas de rendu
***** with robotschool
il y a du rendu 
*Warning* j'ai du installé une librairie manquante
sudo apt-get install libpcre16-3 
*** 05/09/2019
**** connection par ssh avec partage de dossier (hors console) 
 nautilus sftp://tanne@k2so
ssh tanne-local@k2so  (pour se co sur la machine) 
ssh -X tanne-local@k2so (pour se co en permettant le server X et donc les rendues)
**** activer coànda
exec bash

*** 06/09/2019
**** PETS with 5-dof arm
params: 500 population, 50 episode length, 5 planning horizon
It works with different goals and the same initial state

*** 09/09/2019
**** HC
roboschool donne un example de controller performant, la période de marche est d'environ 25-30 steps (d'où le 30 steps d'horizon).
***** RPY
Roll, tourner autour de son axe longitudinale
Pitch, picker ou monté du nez
Yaw, tanguer a gauche ou à droite
***** observation space
 (x_t - x_t-1)/ dt
z
y *seems to be constantly null* 
joints_angles (6)
vx
vz
vy
joints_velocities (6)
***** changes in the files
****** gym_mujoco_walkers
I changed the obs dim from 26 to 16
I add in the step return as info the triple (x,y,z) from self.robot_body_pose().xyz() in gym_forward_walker
*** 11/09/2019
**** rendering with pybullet 
put the rendering before the reset
*** 12/09/2019
**** PETS params
marche:
python mbexp.py -ca opt-type Random -o ctrl_cfg.prop_cfg.model_pretrained True 
[[file:///home/timothee/Videos/HCworking_100.mp4][video]] 100 steps, [[file:///home/timothee/Videos/HCworking_1000.mp4][video]] 1000 steps
**** gym video recording for the recent version
change 
from gym.monitoring import VideoRecorder
into
from gym.wrappers.monitoring.video_recorder import VideoRecorder
*** 13/09/2019
**** Rendering with pybullet
to have the camera following the agent, I looked at the rendering function of pybulletgym 
(pybullet-gym/pybulletgym/envs/mujoco/envs/env_baseswhich is in the installer fodler and not in the installed folder) and the camera is 
translated follwing the robot.body_xyz which was always 0,0,0 so i added in my robot class
*self.body_xyz = [qpos.flat[0], qpos.flat[2], qpos.flat[1]]* in the cals_state
***** camera adjust
As the camera move_and_look_at needed the _p (pybullet env) when you call video_recorder.camera_adjust(), I give it in _reset 
**** dt
in mujoco it's dt=0.05s (20fps), in pybullet dt=0.0165 (64fps), I change the timestep and framskip from (0.004125, 4) to (0.005, 10) 
in WalkerBaseMijocoEnv.create_single_player_scene
**** deprecated registration in gym env for Mujoco HC
lib/python3.6/site-packages/gym/envs/registration.py
I changed load(false) into resolve()

*** 16/09/2019
**** Grid500
ssh tanne@access.grid5000.fr (mdp work)
ssh nancy
exec bash
***** SSH
I had to create a new ssh key that i have added on github
*Warning* mdp inria work
**** Half cheetah
I abandoned the use of the environment on pybullet, to much work to make it work
**** Ant 
I used the Ant environment, it works first run (>1100 reward) 
The replaying is not identicall due to float precision in the action replayed
 
*** 17/09/2019
**** ssh passphrase reasking on git for grid5000
in .ssh:
eval `ssh-agent -s`
then:
ssh-add id_rsa

**** using the conda env after connecting to node in ionteractive mode
export PATH=/home/tanne/miniconda3/ens/chua/bin:$PATH
exec bash
conda activate chua

**** connect to graffiti 
oarsub -q production -p "cluster='graffiti'" -l gpu=1 -I


**** know jobs
oarstat -u tanne

**** example from resibots wiki 
[[https://gitlab.inria.fr/resibots/docs/wikis/reference/Cluster][example]] 

**** create conda env for graffiti tensorflow-gpu
conda create -n ENVNAME [[./conda_env_graffiti.txt][lib version]]

**** tmux
ctrl + b suivi de d pour detacher
tmux a -t SessionName

**** Expé
j'ai lancé PETS sur Ant (150ep) avec (D,cem), (DE,cem), (PE,random), (D,random)
avec la commande (une fois sur la machine)
./my_script.sh graffiti mbexp.py -ca model-typ D -ca prop-type E -ca opt-type Random
*** 18/09/2019
**** g5k configuration
***** ssh
I follow the quick configuration steps [[https://www.grid5000.fr/w/SSH#The_Grid.275000_case][g5k ssh]]
**** what works for now
#in my local script folder;
./g5k_script.sh reservation *N_node* *Walltime*
#now connected in nancy.g5k
oarsub -C *JOBID*
#now connected in the job
cd Path/To/Script
./g5k_script.sh run *conda env* *path to python file*
#example:
./g5k_script.sh run graffiti ./Documents/Prelab_LARSEN/handful-of-trials/scripts/mbexp.py 
**** what works better (thanks to Raj)
ssh nancy.g5k
# for git pull with ssh
 eval `ssh-agent -s`
 ssh-add .ssh/id_rsa
cd Documents/Prelab_LARSEN/handful-of-trials/scripts/
python python_oarcall.py
# for convenience
watch oarstat -u tanne
*** 19/09/2019
**** Minitaur
***** rendering
put render=True in the gym.make()
***** observation
****** noise
the default noise is zero
****** position
I have added the xyz position at the end of the observation
***** reward
the reward is the step distance in the x axis, i also added a survival cost to avoid the minotaur to flip over et move with the wheel of the motor
**** Python path
# put this at the satrt of each file
currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))
parentdir = os.path.dirname(os.path.dirname(currentdir))
os.sys.path.insert(0, parentdir)

**** PETS HC 
only work for PE-CEM, no significative difference between D, DE, CEM/Random, sampling/full 
*** 20/09/2019
**** Minitaur
it can learn to move forward or backwar
it often fall over and start moving using the wheels
the actions are the desired motor angles
*** 23/09/2019
**** my exps
I had to modify the config gile because the sol_dim was not dynamically link to the horizon
**** learning to adapt
***** installation
conda create -n adapt tensorflow-gpu==1.13.1bre
pip install -r requirement.txt #I created the requirment.txt from the yml file
#install on laptop give mpi error
brew isntall mpich #still an error
sudo apt install libopenmpi-dev #worked 

***** code error
the first env is initialized with reset_every_episode=True but for the env created for the vectoriel parallelisation of the envs, th edep copy creates envs with reset_every_episode=False
*** 24/09/2019
**** Minitaur
***** Horizon
12h is not enough for 30H/1000steps/300episodes (260/300)
H=10 seemes significally better and H=30 significally worst
***** Energy
0.0005 successfully reduce the action norm (compare to random and basic) but it also reduce the x reward (compare to basic but is still better than random)
***** Survival
No significative difference, seems to fall less with energy cost, average of 60%. 
**** Meta-learning
***** Raj:
****** 1- train a handful-of-trial model from each environment to collect data
****** 2- train a meta-learner from all the collected data -> one theta*  
****** 3- face to a new environment, it collect an episode using theta* and update theta* on this data and repeat at each episode 
***** Adapt
****** 1- collect data using the current theta with one gradient step on the previous M steps
****** 2- update theta using all/sample from the (t-M:t-1,t:t+K) chunk of trajectories
****** 3- face to a testing environment, at eahc timestep it will update from theta* with one or few gradient step to selec the actio
*** 25/09/2019
**** Minitaur
***** Horizon 
*Warning* H=15 false parameter in truce H=5
H<=15: went farther away but fall in 80% of the case
H>=20: went closer away but fall in 60 of the case
if I nullify the reward if it has fallen, 20 and 30 are equivalent even if 30 fall a bit less
***** Reward function  
slightly better with survival weight of 10
***** Action
the dynamic must be too difficult for the model to learn it, solutions:
-repeat actions to simplify the duynmaic
*** 26/09/2019
**** Minitaur
***** Horizon
15 seems the best for the farther to fall, but 30 fall less
***** Survival cost
does not change much
***** Max velocity limit
- 100 does nto move and does nto fall < 2% [[file:///home/timothee/Documents/Prelab_LARSEN/fast_adaptation_embedding/exp/log/Saved/Minitaur/2019-09-25--17:01:5611535/iter_289.mp4][video]]
- 150 move enough, same last step reward et fall <10% [[file:///home/timothee/Documents/Prelab_LARSEN/fast_adaptation_embedding/exp/log/Saved/Minitaur/2019-09-25--17:53:3030845/iter_299.mp4][video]] 
- 200 fall >30% 
- 300 = inf, fall >60% [[file:///home/timothee/Documents/Prelab_LARSEN/fast_adaptation_embedding/exp/log/Saved/Minitaur/2019-09-25--20:19:0345732/iter_299.mp4][video]] 

*** 27/09/2019
**** Motor velocity limit
150 does not fall much 5%
175 fall 20% but fo farther
**** MVL150 with different Horizon
***** Fall
H10 > H20 > H30
***** last step
H10 > H20 > H30
**** MVL 150, angle limit 
***** fall
50 still fall the same (5%), 33% does not fall [[file:///home/timothee/Documents/Prelab_LARSEN/fast_adaptation_embedding/exp/log/Saved/Minitaur/2019-09-26--16:57:5792342/iter_199.mp4][33%]] [[file:///home/timothee/Documents/Prelab_LARSEN/fast_adaptation_embedding/exp/log/Saved/Minitaur/2019-09-26--16:57:5793716/iter_199.mp4][50%]]
***** last step 
50% et 33% better than no limit 
**** MVL 150, K
K>1 = 100% fall [[file:///home/timothee/Documents/Prelab_LARSEN/fast_adaptation_embedding/exp/log/Saved/Minitaur/2019-09-26--16:57:4587438/iter_179.mp4][K5]] 
**** Random shooting
the population of 500 does not use all of the gpu but increasing the population to 2000 does not increase the usage of the GPU but increase proportionnaly the time needed to select the action
*** 30/09/2019
**** exp
***** Horizon 50
error:  
  File "exp/minitaur_env_adaptation.py", line 461, in <module>
    main(args.config, args.logdir)
  File "exp/minitaur_env_adaptation.py", line 414, in main
    samples=samples)
  File "exp/minitaur_env_adaptation.py", line 210, in execute_2
    recorder.capture_frame()
  File "/home/tanne/Documents/Prelab_LARSEN/gym/gym/wrappers/monitoring/video_recorder.py", line 116, in capture_frame
    self._encode_image_frame(frame)
  File "/home/tanne/Documents/Prelab_LARSEN/gym/gym/wrappers/monitoring/video_recorder.py", line 166, in _encode_image_frame
    self.encoder.capture_frame(frame)
  File "/home/tanne/Documents/Prelab_LARSEN/gym/gym/wrappers/monitoring/video_recorder.py", line 303, in capture_frame
    self.proc.stdin.write(frame.tobytes())
BrokenPipeError: [Errno 32] Broken pipe
***** Horizon comparison with angle 50%
****** Fall 
30 seems better
***** Layer size (100,200,800)
no significative differences
***** Model_error
[[file:///home/timothee/Documents/Prelab_LARSEN/Documents/Figures/Minitaur/first%20expe%20on%20simulation/comparison_LS_model_error.png][png]] in 20 episodes the error does no progress
** October
*** 01/10/2019
**** comparions Popsize in random shooting (1k,4k,8k,16k) with movement velocity 150 and action between -0.5,0.5
no differences, fall < 2% bu reward = 1
time: 3h40 - 4h10 - 4h40 - 5h30
[[file:///home/timothee/Documents/Prelab_LARSEN/fast_adaptation_embedding/exp/log/Saved/Minitaur/2019-09-30--16:40:582204/iter_199.mp4][1k]] [[file:///home/timothee/Documents/Prelab_LARSEN/fast_adaptation_embedding/exp/log/Saved/Minitaur/2019-09-30--16:39:1612337/iter_199.mp4][16k]] 
**** CMA-ES
pip install cma
xopt, es = cma.fmin2(None, np.zeros(config["sol_dim"]), 0.5,
                     parallel_objective=lambda x: list(config["cost_fn"](x)),
                     options={'maxfevals': config['max_iters']*config['popsize'], 'popsize': config['popsize']}
                    )
sol = xopt
*** 02/10/2019
**** gym minitaur env
it needs tensorflow for logging
*** 07/10/2019
**** Minitaur
We need to use the accurate_motor_model_enable, it improve the smoothness and bring better model from the NN.
[[file:///home/timothee/Documents/Prelab_LARSEN/Documents/Figures/Minitaur/Second%20exp%20on%20gymMinitaur/error_model_accurate.png][model error]] [[file:///home/timothee/Documents/Prelab_LARSEN/Documents/Figures/Minitaur/Second%20exp%20on%20gymMinitaur/reward_model_accurate.png][reward]]
[[file:///home/timothee/Documents/Prelab_LARSEN/fast_adaptation_embedding/exp/log/Saved/GymMinitaur/2019-10-04--15:19:2587318/iter_99.mp4][video 99]] [[file:///home/timothee/Documents/Prelab_LARSEN/fast_adaptation_embedding/exp/log/Saved/GymMinitaur/2019-10-04--15:19:2587318/iter_299.mp4][video 299 (200)]]
***** Popsize
Increasing by 100 the population of the MPC optimizer (RS) does not improve the reward see figures above.
We could conclude that the default parameters for the RS 12500 individuals for the first step followed by 500 individuals are enough to take the best of the model.
As there is also no overfitting of the model 
***** episode length
passing from 250 to 500 icnrease the fall percentage to 20% and do not increase much the average reward. 
The model seems to be worst.
***** model size
the default size [200,200,100] seems to be the best one compared to
[50,50,50], [100,100,50] and [400,400,200] [[file:///home/timothee/Documents/Prelab_LARSEN/Documents/Figures/Minitaur/Second%20exp%20on%20gymMinitaur/compare_model_size_reward.png][reward]] [[file:///home/timothee/Documents/Prelab_LARSEN/Documents/Figures/Minitaur/Second%20exp%20on%20gymMinitaur/compare_model_size_error.png][error]]
*** 08/10/2019
**** nothing
**** survival
it seems to be the worst parameter (maybe the model overestimate the falls)
[[file:///home/timothee/Documents/Prelab_LARSEN/fast_adaptation_embedding/exp/log/Saved/GymMinitaur/2019-10-08--09:41:1315203/iter_99.mp4][video]] 
to much vibration
**** drift 
better but maybe to much enphasis on the drift part [[file:///home/timothee/Documents/Prelab_LARSEN/fast_adaptation_embedding/exp/log/Saved/GymMinitaur/2019-10-08--10:05:0427108/iter_99.mp4][video]]
**** skake + drift 
good results [[file:///home/timothee/Documents/Prelab_LARSEN/fast_adaptation_embedding/exp/log/Saved/GymMinitaur/2019-10-08--10:40:5574062/iter_99.mp4][video]]
*** 09/10/2019
**** Shake,drift,survival
with a gait frequency of 2 Hz, using half a gait of horizon does not five very good result.
drift seems to avoid some fall
*** 10/10/2019
**** H50/discount/ensembles
200 iterations * 500 episode length
H50 discount (4h) VS E5 H50 discount (12h) VS E5 H25 discount (8h) 
[[file:///home/timothee/Documents/Prelab_LARSEN/fast_adaptation_embedding/exp/log/Saved/GymMinitaur/2019-10-09--16:31:1253463/iter_149.mp4][video H50 discount]] r=1.05 
H50 discount has better reward in average and good result in max (1.56) but it falls more often
using an ensemble seems to decrease the reward and the percentage of fall from 60% to 40%
The testing error follow nicely the training error
**** The difference in the motor action sent and realised
[[file:///home/timothee/Documents/Prelab_LARSEN/Documents/Figures/Minitaur/Second%20exp%20on%20gymMinitaur/motor_actions_differences.png][motor joints]] of tjhe previous video
the vibrations occurs when the legs is not in contact with the floor
**** Ensemble
with a sampling of 10% they start to present disimilarity
with a sampling of 25%, they were the same
*** 11/10/2019
Evaluation of the model error on the data.
be carefull about the normailization
I added the R² coefficient
