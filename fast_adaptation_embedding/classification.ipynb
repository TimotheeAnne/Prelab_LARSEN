{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_CM(CM, toprint=False):\n",
    "    [[TP, FP], [FN, TN]] = CM\n",
    "    TPR = TP/(TP+FN) if (TP +FN) !=0 else 0\n",
    "    TNR = TN/(TN+FP) if (FP +TN) !=0 else 0\n",
    "    ACC = (TP+TN)/(TP+TN+FP+FN) if (TP+TN+FP+FN) !=0 else 0\n",
    "    Precision = TP/(TP+FP) if (TP+FP) !=0 else 0\n",
    "    FS = 2*TP/(2*TP+FP+FN) if (2*TP+FP+FN) !=0 else 0\n",
    "    if toprint:\n",
    "        print(\"TPR: \",TPR)\n",
    "        print(\"TNR: \", TNR)\n",
    "        print(\"ACC: \", ACC)\n",
    "        print(\"Precision: \", Precision)\n",
    "        print(\"FS: \",FS)\n",
    "    return [TPR,TNR, ACC,Precision,FS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97483/97483 [00:00<00:00, 217178.30it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('data/train_fallen.pk', 'rb') as f:\n",
    "    train_samples = pickle.load(f)\n",
    "obs = np.array(train_samples['obs'])\n",
    "controller = np.array(train_samples['controller'])\n",
    "t0 = train_samples['t0']\n",
    "X, y = [], []\n",
    "for i in tqdm(range(len(obs))):\n",
    "    X.append(np.concatenate((obs[i,0,:28], obs[i,0,30:31], controller[i], [(t0[i]%20)/20])))\n",
    "    y.append(obs[i][-1][30]>0.13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22006/22006 [00:00<00:00, 196000.67it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('data/eval_fallen.pk', 'rb') as f:\n",
    "    samples = pickle.load(f)\n",
    "obs = np.array(samples['obs'])\n",
    "controller = np.array(samples['controller'])\n",
    "t0 = samples['t0']\n",
    "X_t, y_t = [], []\n",
    "for i in tqdm(range(len(obs))):\n",
    "    X_t.append(np.concatenate((obs[i,0,:28], obs[i,0,30:31], controller[i], [(t0[i]%20)/20])))\n",
    "    y_t.append(obs[i][-1][30]>0.13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "opitmizer = ['lbfgs', 'sgd', 'adam'][2]\n",
    "clf = MLPClassifier(solver=opitmizer, alpha=1e-5,hidden_layer_sizes=(50,50), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.61595610\n",
      "Iteration 2, loss = 0.52651492\n",
      "Iteration 3, loss = 0.45208965\n",
      "Iteration 4, loss = 0.42025503\n",
      "Iteration 5, loss = 0.40718764\n",
      "Iteration 6, loss = 0.39484627\n",
      "Iteration 7, loss = 0.38784355\n",
      "Iteration 8, loss = 0.37990830\n",
      "Iteration 9, loss = 0.37389550\n",
      "Iteration 10, loss = 0.36935346\n",
      "Iteration 11, loss = 0.36412122\n",
      "Iteration 12, loss = 0.35954774\n",
      "Iteration 13, loss = 0.35558724\n",
      "Iteration 14, loss = 0.35187834\n",
      "Iteration 15, loss = 0.34815411\n",
      "Iteration 16, loss = 0.34533471\n",
      "Iteration 17, loss = 0.34244130\n",
      "Iteration 18, loss = 0.33848096\n",
      "Iteration 19, loss = 0.33426869\n",
      "Iteration 20, loss = 0.33094135\n",
      "Iteration 21, loss = 0.32739018\n",
      "Iteration 22, loss = 0.32313631\n",
      "Iteration 23, loss = 0.32051338\n",
      "Iteration 24, loss = 0.31629527\n",
      "Iteration 25, loss = 0.31355569\n",
      "Iteration 26, loss = 0.31221019\n",
      "Iteration 27, loss = 0.30840788\n",
      "Iteration 28, loss = 0.30764387\n",
      "Iteration 29, loss = 0.30414336\n",
      "Iteration 30, loss = 0.30226525\n",
      "Iteration 31, loss = 0.30019138\n",
      "Iteration 32, loss = 0.29733314\n",
      "Iteration 33, loss = 0.29587737\n",
      "Iteration 34, loss = 0.29325932\n",
      "Iteration 35, loss = 0.29157016\n",
      "Iteration 36, loss = 0.29080707\n",
      "Iteration 37, loss = 0.28868645\n",
      "Iteration 38, loss = 0.28679629\n",
      "Iteration 39, loss = 0.28553959\n",
      "Iteration 40, loss = 0.28376800\n",
      "Iteration 41, loss = 0.28232697\n",
      "Iteration 42, loss = 0.28093266\n",
      "Iteration 43, loss = 0.27873083\n",
      "Iteration 44, loss = 0.27795359\n",
      "Iteration 45, loss = 0.27810594\n",
      "Iteration 46, loss = 0.27702248\n",
      "Iteration 47, loss = 0.27528254\n",
      "Iteration 48, loss = 0.27464121\n",
      "Iteration 49, loss = 0.27395475\n",
      "Iteration 50, loss = 0.27112099\n",
      "Iteration 51, loss = 0.27036525\n",
      "Iteration 52, loss = 0.27036346\n",
      "Iteration 53, loss = 0.26939186\n",
      "Iteration 54, loss = 0.26877702\n",
      "Iteration 55, loss = 0.26835128\n",
      "Iteration 56, loss = 0.26685532\n",
      "Iteration 57, loss = 0.26720182\n",
      "Iteration 58, loss = 0.26501393\n",
      "Iteration 59, loss = 0.26421707\n",
      "Iteration 60, loss = 0.26469412\n",
      "Iteration 61, loss = 0.26363689\n",
      "Iteration 62, loss = 0.26288022\n",
      "Iteration 63, loss = 0.26173892\n",
      "Iteration 64, loss = 0.26217577\n",
      "Iteration 65, loss = 0.26073996\n",
      "Iteration 66, loss = 0.25981837\n",
      "Iteration 67, loss = 0.26053987\n",
      "Iteration 68, loss = 0.25935797\n",
      "Iteration 69, loss = 0.25823942\n",
      "Iteration 70, loss = 0.25763097\n",
      "Iteration 71, loss = 0.25694946\n",
      "Iteration 72, loss = 0.25741152\n",
      "Iteration 73, loss = 0.25638138\n",
      "Iteration 74, loss = 0.25675899\n",
      "Iteration 75, loss = 0.25522671\n",
      "Iteration 76, loss = 0.25575378\n",
      "Iteration 77, loss = 0.25453827\n",
      "Iteration 78, loss = 0.25556877\n",
      "Iteration 79, loss = 0.25334007\n",
      "Iteration 80, loss = 0.25343707\n",
      "Iteration 81, loss = 0.25265237\n",
      "Iteration 82, loss = 0.25377136\n",
      "Iteration 83, loss = 0.25236051\n",
      "Iteration 84, loss = 0.25091762\n",
      "Iteration 85, loss = 0.25099276\n",
      "Iteration 86, loss = 0.25069335\n",
      "Iteration 87, loss = 0.25074097\n",
      "Iteration 88, loss = 0.24934329\n",
      "Iteration 89, loss = 0.24919687\n",
      "Iteration 90, loss = 0.24968487\n",
      "Iteration 91, loss = 0.24982307\n",
      "Iteration 92, loss = 0.24875245\n",
      "Iteration 93, loss = 0.24923451\n",
      "Iteration 94, loss = 0.24894567\n",
      "Iteration 95, loss = 0.24761952\n",
      "Iteration 96, loss = 0.24807077\n",
      "Iteration 97, loss = 0.24715529\n",
      "Iteration 98, loss = 0.24637683\n",
      "Iteration 99, loss = 0.24723194\n",
      "Iteration 100, loss = 0.24663693\n",
      "Iteration 101, loss = 0.24647382\n",
      "Iteration 102, loss = 0.24592775\n",
      "Iteration 103, loss = 0.24524833\n",
      "Iteration 104, loss = 0.24547847\n",
      "Iteration 105, loss = 0.24532736\n",
      "Iteration 106, loss = 0.24575825\n",
      "Iteration 107, loss = 0.24433880\n",
      "Iteration 108, loss = 0.24469495\n",
      "Iteration 109, loss = 0.24408757\n",
      "Iteration 110, loss = 0.24397204\n",
      "Iteration 111, loss = 0.24436690\n",
      "Iteration 112, loss = 0.24298135\n",
      "Iteration 113, loss = 0.24311188\n",
      "Iteration 114, loss = 0.24315938\n",
      "Iteration 115, loss = 0.24211159\n",
      "Iteration 116, loss = 0.24339309\n",
      "Iteration 117, loss = 0.24203707\n",
      "Iteration 118, loss = 0.24240373\n",
      "Iteration 119, loss = 0.24063747\n",
      "Iteration 120, loss = 0.24131047\n",
      "Iteration 121, loss = 0.24092233\n",
      "Iteration 122, loss = 0.24135818\n",
      "Iteration 123, loss = 0.24026249\n",
      "Iteration 124, loss = 0.24121417\n",
      "Iteration 125, loss = 0.24102099\n",
      "Iteration 126, loss = 0.23999185\n",
      "Iteration 127, loss = 0.23961138\n",
      "Iteration 128, loss = 0.23981634\n",
      "Iteration 129, loss = 0.24007285\n",
      "Iteration 130, loss = 0.23997149\n",
      "Iteration 131, loss = 0.23950518\n",
      "Iteration 132, loss = 0.23873176\n",
      "Iteration 133, loss = 0.23844000\n",
      "Iteration 134, loss = 0.23732909\n",
      "Iteration 135, loss = 0.23761759\n",
      "Iteration 136, loss = 0.23764353\n",
      "Iteration 137, loss = 0.23704290\n",
      "Iteration 138, loss = 0.23729505\n",
      "Iteration 139, loss = 0.23770055\n",
      "Iteration 140, loss = 0.23781162\n",
      "Iteration 141, loss = 0.23768568\n",
      "Iteration 142, loss = 0.23663641\n",
      "Iteration 143, loss = 0.23644093\n",
      "Iteration 144, loss = 0.23726384\n",
      "Iteration 145, loss = 0.23614324\n",
      "Iteration 146, loss = 0.23662934\n",
      "Iteration 147, loss = 0.23565343\n",
      "Iteration 148, loss = 0.23662872\n",
      "Iteration 149, loss = 0.23517237\n",
      "Iteration 150, loss = 0.23568806\n",
      "Iteration 151, loss = 0.23530538\n",
      "Iteration 152, loss = 0.23515200\n",
      "Iteration 153, loss = 0.23575175\n",
      "Iteration 154, loss = 0.23518092\n",
      "Iteration 155, loss = 0.23408462\n",
      "Iteration 156, loss = 0.23438722\n",
      "Iteration 157, loss = 0.23342870\n",
      "Iteration 158, loss = 0.23460190\n",
      "Iteration 159, loss = 0.23353063\n",
      "Iteration 160, loss = 0.23438611\n",
      "Iteration 161, loss = 0.23400818\n",
      "Iteration 162, loss = 0.23357831\n",
      "Iteration 163, loss = 0.23379660\n",
      "Iteration 164, loss = 0.23408113\n",
      "Iteration 165, loss = 0.23290878\n",
      "Iteration 166, loss = 0.23198884\n",
      "Iteration 167, loss = 0.23356565\n",
      "Iteration 168, loss = 0.23211568\n",
      "Iteration 169, loss = 0.23148761\n",
      "Iteration 170, loss = 0.23270846\n",
      "Iteration 171, loss = 0.23238694\n",
      "Iteration 172, loss = 0.23242484\n",
      "Iteration 173, loss = 0.23117776\n",
      "Iteration 174, loss = 0.23207668\n",
      "Iteration 175, loss = 0.23239622\n",
      "Iteration 176, loss = 0.23179367\n",
      "Iteration 177, loss = 0.23112630\n",
      "Iteration 178, loss = 0.23151614\n",
      "Iteration 179, loss = 0.23085045\n",
      "Iteration 180, loss = 0.23154711\n",
      "Iteration 181, loss = 0.23093417\n",
      "Iteration 182, loss = 0.23073092\n",
      "Iteration 183, loss = 0.23018641\n",
      "Iteration 184, loss = 0.22985815\n",
      "Iteration 185, loss = 0.23063409\n",
      "Iteration 186, loss = 0.22869474\n",
      "Iteration 187, loss = 0.23074251\n",
      "Iteration 188, loss = 0.22904106\n",
      "Iteration 189, loss = 0.23003076\n",
      "Iteration 190, loss = 0.23054140\n",
      "Iteration 191, loss = 0.22924757\n",
      "Iteration 192, loss = 0.22945498\n",
      "Iteration 193, loss = 0.22966211\n",
      "Iteration 194, loss = 0.22851307\n",
      "Iteration 195, loss = 0.22791739\n",
      "Iteration 196, loss = 0.22922812\n",
      "Iteration 197, loss = 0.22807362\n",
      "Iteration 198, loss = 0.22892473\n",
      "Iteration 199, loss = 0.22805802\n",
      "Iteration 200, loss = 0.22763369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timothee/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(50, 50), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "TPR:  0.9167493570401571\n",
      "TNR:  0.8964492120909734\n",
      "ACC:  0.9067324559153904\n",
      "Precision:  0.9008775944757318\n",
      "FS:  0.9087441785771639\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X)\n",
    "\n",
    "train_CM = [[0,0],[0,0]]\n",
    "for i in range(len(y)):\n",
    "    t = y[i]\n",
    "    p = pred[i]\n",
    "    train_CM[int(p)][int(t)] += 1\n",
    "print('Train')\n",
    "analyse_CM(train_CM, True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval\n",
      "TPR:  0.8776213731686298\n",
      "TNR:  0.8584277436651389\n",
      "ACC:  0.8675361265109516\n",
      "Precision:  0.8484539900018515\n",
      "FS:  0.8627912449988232\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_t)\n",
    "\n",
    "eval_CM = [[0,0],[0,0]]\n",
    "for i in range(len(y_t)):\n",
    "    t = y_t[i]\n",
    "    p = pred[i]\n",
    "    eval_CM[int(p)][int(t)] += 1\n",
    "print('eval')\n",
    "analyse_CM(eval_CM, True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
